{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 検索データ作成\n",
    "\n",
    "[Natsume](https://hinoki-project.org/natsume/)は，名詞ー格助詞ー動詞などの構文パターンを検索したり，そのジャンル間の使用を比較したりすることができるシステムです。\n",
    "ここでは，その検索機能の一つ，名詞ー格助詞ー動詞の構文パターンを抽出することにします。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 準備\n",
    "\n",
    "すでに`requirements.txt`を`pip`などでインストール済みだったら，以下の宣言は不要です。Google Colab などの場合は実行が必要になります（`#`をとってください）。\n",
    "\n",
    "DH ラボの iMac はここで`pip3`を使ってください。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ![[ ! -d natume-simple ]] && git clone https://github.com/borh/natsume-simple.git\n",
    "# !cd natsume-simple && git pull\n",
    "# !mv natsume-simple/* .\n",
    "# !pip install -r requirements.txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Ginza\n",
    "\n",
    "Ginza は SpaCy を裏で使っているので，SpaCy と使用がほとんど変わりません。ただし，一部追加機能があります。\n",
    "追加機能は主に文節処理とトーケン（形態素）のレマ（語彙素）の参照方法です。詳しくは[公式サイトへ](https://megagonlabs.github.io/ginza/)。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ginza\n",
    "import spacy\n",
    "\n",
    "try:\n",
    "    is_using_gpu = (\n",
    "        spacy.prefer_gpu()\n",
    "    )  # GPUがあれば，使う # GPU搭載で強制的にCPUを使う場合は，ここでFalseに変える。\n",
    "except Exception:\n",
    "    is_using_gpu = False\n",
    "\n",
    "\n",
    "if is_using_gpu:\n",
    "    print(\"Using GPU\")\n",
    "\n",
    "\n",
    "FORCE_MODEL = False  # ここを任意モデル文字列にすれば設定できる\n",
    "\n",
    "if FORCE_MODEL:\n",
    "    model_name = FORCE_MODEL\n",
    "    nlp = spacy.load(model_name)\n",
    "else:\n",
    "    try:\n",
    "        model_name = \"ja_ginza_bert_large\"\n",
    "        nlp = spacy.load(model_name)  # あればja_ginza_bert_largeを使用\n",
    "    except Exception:\n",
    "        model_name = \"ja_ginza\"\n",
    "        nlp = spacy.load(model_name)  # なけらばja_ginza\n",
    "\n",
    "ginza.force_using_normalized_form_as_lemma(False)  # lemmaとnormを区別するため\n",
    "# この設定では，normが語彙素に当たり，lemmaが基本形に当たる\n",
    "\n",
    "example_sentence = \"東京では，銀座でランチをたべよう。\"\n",
    "doc = nlp(example_sentence)\n",
    "[\n",
    "    (\n",
    "        token.i,\n",
    "        token.orth_,\n",
    "        token.lemma_,\n",
    "        token.norm_,\n",
    "        token.pos_,\n",
    "        token.tag_,\n",
    "        token.dep_,\n",
    "        token.head.i,\n",
    "        ginza.inflection(token),\n",
    "    )\n",
    "    for token in doc\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 係り受けの例\n",
    "\n",
    "今回対象としている名詞ー格助詞ー動詞（NPV）パターンは係り受け構造の中で，どのように現れるか，簡単な例で示せます。\n",
    "SpaCy/Ginza で使用される係り受け構造の定義は[Universal Dependencies 2](https://universaldependencies.org/u/overview/syntax.html)と[論文](https://aclanthology.org/L18-1287.pdf)をご参照ください。\n",
    "\n",
    "上記の例でもわかるように，名詞ー格助詞ー動詞でナイーブな抽出を行うと，「東京で食べる」「銀座で食べる」「ランチを食べる」の３共起表現が抽出されます。ただし，その中の「東京で食べる」は実は「で」格助詞単独ではなく，「では」という連語の形で出現します。\n",
    "goo 辞書の解説では[以下の通り](https://dictionary.goo.ne.jp/word/%E3%81%A7%E3%81%AF/)定義されています：\n",
    "\n",
    "```\n",
    " で‐は の解説\n",
    "\n",
    "［連語］\n",
    "《断定の助動詞「だ」の連用形＋係助詞「は」》判断の前提を表す。…であるとすれば。…だと。「雨では中止になる」「彼ではだれも承知しないだろう」\n",
    "《格助詞「で」＋係助詞「は」》…で。…においては。…を用いては。「今日では問題にされない」\n",
    "《接続助詞「で」＋係助詞「は」》未然形に付く。\n",
    "[...]\n",
    "```\n",
    "\n",
    "他にも「でも」「へと」「へは」「からは」など複合助詞が存在し，単独係助詞よりは文法的な役割が複雑なため，今回で検索対象から外すようにします。\n",
    "\n",
    "### 係り受け関係の可視化\n",
    "\n",
    "Cabocha や KNP のように文節を係り受けの単位にしているものと違い，SpaCy/GiNZA ではトーケン（形態素）を単位として係り受け関係を表しています。\n",
    "そのため，長文になればなるほど，その構造が最初の例（print 関数などを使う）より読みにくくなってしまいます。\n",
    "そのため，SpaCy では可視化ツール displacy を用意しています。\n",
    "\n",
    "よく使うので，最初にヘルパー関数 pp を定義し，文字列を入力として簡単にかかり受け図を出力するようにしておきます。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "# https://spacy.io/api/top-level#displacy_options\n",
    "compact = {\n",
    "    \"compact\": True,\n",
    "    \"add_lemma\": True,\n",
    "    \"distance\": 100,\n",
    "    \"word_spacing\": 30,\n",
    "    \"color\": \"#ffffff\",\n",
    "    \"bg\": \"#1e1e1e\",  # dark modeでない場合は，コメントアウト\n",
    "}  # 表示を長い文用に工夫\n",
    "\n",
    "\n",
    "def pp(s: str):\n",
    "    return displacy.render(nlp(s), options=compact, jupyter=True)\n",
    "\n",
    "\n",
    "pp(\"東京では，銀座でランチを食べよう。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 残念ながら，y幅は調整できない\n",
    "pp(\n",
    "    \"１８紀の哲学者ヒュームは，「力はいつも被治者の側にあり，支配者には自分たちを支えるものは世論以外に何もないということがわかるであろう」と論じているが，仮に選挙がなくとも，大多数の被治者からの暗黙の同意がなければ如何なる政治体制も不安定にならざるを得ないだろう。\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp(\n",
    "    \"さらに，数年おきに選挙が行われるだけではなく，マスメディアが発達し，世論調査が頻繁に行われている現在の状況を考えれば，以前と比べて，民意の重要性は，高まっていると思われる。\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 係り受け関係の抽出\n",
    "\n",
    "係り受け関係は SpaCy で DependencyMatcher という機能で検索できます。\n",
    "\n",
    "- <https://spacy.io/usage/rule-based-matching#dependencymatcher>\n",
    "\n",
    "Semgrex の記号を使うことによって，係り受け構造の定義がわりと自由にできます。\n",
    "\n",
    "```\n",
    "SYMBOL\tDESCRIPTION\n",
    "A < B\tA is the immediate dependent of B.\n",
    "A > B\tA is the immediate head of B.\n",
    "A << B\tA is the dependent in a chain to B following dep → head paths.\n",
    "A >> B\tA is the head in a chain to B following head → dep paths.\n",
    "A . B\tA immediately precedes B, i.e. A.i == B.i - 1, and both are within the same dependency tree.\n",
    "A .* B\tA precedes B, i.e. A.i < B.i, and both are within the same dependency tree (not in Semgrex).\n",
    "A ; B\tA immediately follows B, i.e. A.i == B.i + 1, and both are within the same dependency tree (not in Semgrex).\n",
    "A ;* B\tA follows B, i.e. A.i > B.i, and both are within the same dependency tree (not in Semgrex).\n",
    "A $+ B\tB is a right immediate sibling of A, i.e. A and B have the same parent and A.i == B.i - 1.\n",
    "A $- B\tB is a left immediate sibling of A, i.e. A and B have the same parent and A.i == B.i + 1.\n",
    "A $++ B\tB is a right sibling of A, i.e. A and B have the same parent and A.i < B.i.\n",
    "A $-- B\tB is a left sibling of A, i.e. A and B have the same parent and A.i > B.i.\n",
    "```\n",
    "\n",
    "DependencyMatcher の利用が向いているのは，検索対象の型が固定であり，マッチングに否定が必要ない時です。\n",
    "しかし，\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import DependencyMatcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "\n",
    "def make_token(\n",
    "    name: str,\n",
    "    attrs: dict[str, Any],\n",
    "    dep_name: str | None = None,\n",
    "    rel_op: str | None = None,\n",
    ") -> dict[str, Any]:\n",
    "    spec = {\n",
    "        \"RIGHT_ID\": name,\n",
    "        \"RIGHT_ATTRS\": attrs,\n",
    "    }\n",
    "    if dep_name and rel_op:\n",
    "        spec[\"LEFT_ID\"] = dep_name\n",
    "        spec[\"REL_OP\"] = rel_op\n",
    "    return spec\n",
    "\n",
    "\n",
    "make_token(\"verb\", {\"POS\": \"VERB\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_token(\"noun\", {\"DEP\": {\"IN\": [\"obj\", \"obl\", \"nsubj\"]}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokens import Doc\n",
    "\n",
    "pattern = [\n",
    "    # Anchor token: VERB\n",
    "    {\"RIGHT_ID\": \"verb\", \"RIGHT_ATTRS\": {\"POS\": \"VERB\"}},\n",
    "    # Dependency relation: VERB -> NOUN\n",
    "    {\n",
    "        \"LEFT_ID\": \"verb\",\n",
    "        \"REL_OP\": \">\",\n",
    "        \"RIGHT_ID\": \"noun\",\n",
    "        \"RIGHT_ATTRS\": {\"DEP\": {\"IN\": [\"obj\", \"obl\", \"nsubj\"]}},\n",
    "    },\n",
    "    # NOUN\n",
    "    {\n",
    "        \"LEFT_ID\": \"noun\",\n",
    "        \"REL_OP\": \">\",\n",
    "        \"RIGHT_ID\": \"case_particle\",\n",
    "        \"RIGHT_ATTRS\": {\n",
    "            \"DEP\": \"case\",\n",
    "            \"LEMMA\": {\"IN\": [\"が\", \"を\", \"に\", \"で\", \"から\", \"より\", \"と\", \"へ\"]},\n",
    "        },\n",
    "    },\n",
    "]\n",
    "\n",
    "\n",
    "def matches_to_npv(doc: Doc, matches: list[tuple[int, list[int]]]):\n",
    "    exclude_matches: set[int] = set()\n",
    "    for i, (match_id, (verb, noun, case_particle)) in enumerate(matches):\n",
    "        # 複数の格助詞が連続で出現する場合は，除外リストに入れて，matchesから外す\n",
    "        if doc[case_particle + 1].pos_ == \"ADP\":\n",
    "            print(\n",
    "                \"Double particle:\", doc[case_particle : case_particle + 2], \"excluding.\"\n",
    "            )\n",
    "            exclude_matches.add(i)\n",
    "    matches = [m for i, m in enumerate(matches) if i not in exclude_matches]\n",
    "    return matches\n",
    "\n",
    "\n",
    "matcher = DependencyMatcher(nlp.vocab)\n",
    "matcher.add(\"NPV\", [pattern])\n",
    "matches = matcher(doc)\n",
    "matches = matches_to_npv(doc, matches)\n",
    "matches  # 出力はmatch_idとn,p,vそれぞれの形態素の位置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/explosion/spaCy/blob/master/spacy/symbols.pyx\n",
    "# GiNZA特有のシンボルcaseがないことに注意。文字列ではなく以下のようにsymbolを使うことで処理が若干早くなる。\n",
    "import re\n",
    "from collections.abc import Iterator\n",
    "from itertools import takewhile, tee  # pairwiseがPython 3.10で登場\n",
    "from typing import Iterable\n",
    "\n",
    "import ginza  # 他のメソッドなどを使う時\n",
    "from ginza import bunsetu_span, inflection\n",
    "from spacy.symbols import (\n",
    "    ADP,\n",
    "    NOUN,\n",
    "    NUM,\n",
    "    PRON,\n",
    "    PROPN,\n",
    "    PUNCT,\n",
    "    SCONJ,\n",
    "    SYM,\n",
    "    VERB,\n",
    "    nsubj,\n",
    "    obj,\n",
    "    obl,\n",
    ")\n",
    "from spacy.tokens import Span, Token\n",
    "\n",
    "\n",
    "def pairwise(iterable: Iterable[Any]) -> Iterator[tuple[Any, Any]]:\n",
    "    # pairwise('ABCDEFG') --> AB BC CD DE EF FG\n",
    "    a, b = tee(iterable)\n",
    "    next(b, None)\n",
    "    return zip(a, b)\n",
    "\n",
    "\n",
    "def simple_lemma(token: Token) -> str:\n",
    "    if token.norm_ == \"為る\":\n",
    "        return \"する\"\n",
    "    elif token.norm_ == \"居る\":\n",
    "        return token.lemma_\n",
    "    elif token.norm_ == \"成る\":\n",
    "        return token.lemma_\n",
    "    elif token.norm_ == \"有る\":\n",
    "        return token.lemma_\n",
    "    else:\n",
    "        return token.norm_\n",
    "\n",
    "\n",
    "def normalize_verb_span(tokens: Doc | Span) -> str | None:\n",
    "    \"\"\"動詞が入っている文節のトーケンを入力として，正規化された動詞の文字列を返す。\n",
    "    現在「ます」「た」は除外とし，基本形に直す処理をしているが，完全にすべての活用の組み合わせに対応していな。\"\"\"\n",
    "    clean_tokens = [\n",
    "        token for token in tokens if token.pos not in {PUNCT, SYM}\n",
    "    ]  # 。「」などが始め，途中，終わりに出現することがあるので除外\n",
    "    clean_tokens = list(\n",
    "        takewhile(\n",
    "            lambda token: token.pos not in {ADP, SCONJ}\n",
    "            and token.norm_ not in {\"から\", \"ため\", \"たり\", \"こと\", \"よう\"},\n",
    "            clean_tokens,\n",
    "        )\n",
    "    )  # いる>>と(ADP)<<いう，「いる>>から(SCONJ)<<」は品詞で除外すると「て」も除外される\n",
    "    if len(clean_tokens) == 1:\n",
    "        return simple_lemma(clean_tokens[0])\n",
    "\n",
    "    normalized_tokens: list[Token] = []\n",
    "    token_pairs: list[tuple[Token, Token]] = list(pairwise(clean_tokens))\n",
    "    for i, (token, next_token) in enumerate(token_pairs):\n",
    "        normalized_tokens.append(token)\n",
    "        if next_token.lemma_ == \"ます\" or next_token.lemma_ == \"た\":\n",
    "            if re.match(r\"^(五|上|下|サ|.変格|助動詞).+\", inflection(token)):\n",
    "                # TODO: ませんでした\n",
    "                break\n",
    "            else:\n",
    "                normalized_tokens.append(nlp(\"する\")[0])\n",
    "                break\n",
    "        elif next_token.lemma_ == \"だ\":  # なら(ば)，説明する>>なら(lemma=だ)<<，\n",
    "            break\n",
    "        elif i == len(token_pairs) - 1:  # ペアが最後の場合はnext_tokenも格納\n",
    "            normalized_tokens.append(next_token)\n",
    "\n",
    "    if len(normalized_tokens) == 1:\n",
    "        return simple_lemma(normalized_tokens[0])\n",
    "\n",
    "    if not normalized_tokens:\n",
    "        return None\n",
    "\n",
    "    stem = normalized_tokens[0]\n",
    "    affixes = normalized_tokens[1:-1]\n",
    "    suffix = normalized_tokens[-1]\n",
    "    return \"{}{}{}\".format(\n",
    "        stem.text,  # .lemma_を使う場合は未然形・連用形など注意する必要あり\n",
    "        \"\".join(t.text for t in affixes),\n",
    "        simple_lemma(suffix),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[inflection(t) for t in nlp(\"語ります\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### テストの活用\n",
    "\n",
    "自然言語はその単語の組み合わせが膨大で，すべてをルールで記載するつもりが例外が出てきて思わぬ結果になることが多いです。\n",
    "ルールあるいはプログラムのアルゴリズム・処理などを検証しながら開発を進みたいときは，Python のテスト機能を活用とよいでしょう。\n",
    "しばしば，ノートブックのセルでの実行結果を見ながら書くよりはテストにおさめて，いかなる変更で，以前できた処理ができなかったりする場合やほしい結果がどの時点で得られたかを早期発見できます。\n",
    "\n",
    "以下では，全箇所が正しく処理されるのに対し，最後は失敗します。\n",
    "（実際は「見られない」が正しいですが，失敗の例として「見られないが」を正解にしています。）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unittest\n",
    "\n",
    "\n",
    "class TestVerbNormalization(unittest.TestCase):\n",
    "    def test_norm(self):\n",
    "        self.assertEqual(normalize_verb_span(nlp(\"いるからで\")), \"いる\")\n",
    "        self.assertEqual(normalize_verb_span(nlp(\"いるという\")), \"いる\")\n",
    "        self.assertEqual(normalize_verb_span(nlp(\"語ります\")), \"語る\")\n",
    "        self.assertEqual(normalize_verb_span(nlp(\"しました。\")), \"する\")\n",
    "        self.assertEqual(normalize_verb_span(nlp(\"作り上げたか\")), \"作り上げる\")\n",
    "        self.assertEqual(\n",
    "            normalize_verb_span(nlp(\"見られなかったが\")), \"見られないが\"\n",
    "        )  # 失敗する\n",
    "\n",
    "\n",
    "# ノートブックの中では以下のようにユニットテストできる：\n",
    "unittest.main(\n",
    "    argv=[\"ignored\", \"-v\", \"TestVerbNormalization.test_norm\"], verbosity=2, exit=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unittest\n",
    "\n",
    "# https://megagonlabs.github.io/ginza/bunsetu_api.html\n",
    "\n",
    "\n",
    "# 最初に作ったnpv_matcherを分節ベース処理に書き換える\n",
    "def npv_matcher(doc: Doc) -> list[tuple[str, str, str]]:\n",
    "    matches: list[tuple[str, str, str]] = []\n",
    "    for token in doc[\n",
    "        :-2\n",
    "    ]:  # 検索対象の最小トーケン数が３のため，最後の2トーケンは見なくて良い\n",
    "        noun = token\n",
    "        case_particle = noun.nbor(1)\n",
    "        verb = token.head\n",
    "        if (\n",
    "            noun.pos in {NOUN, PROPN, PRON, NUM}\n",
    "            and noun.dep in {obj, obl, nsubj}\n",
    "            and verb.pos == VERB\n",
    "            and case_particle.dep_ == \"case\"\n",
    "            and case_particle.lemma_\n",
    "            in {\"が\", \"を\", \"に\", \"で\", \"から\", \"より\", \"と\", \"へ\"}\n",
    "            and case_particle.nbor().dep_ != \"fixed\"\n",
    "            and case_particle.nbor().head != case_particle.head\n",
    "        ):  # では，には，をも，へとなどを除外\n",
    "            verb_bunsetu_span = bunsetu_span(verb)\n",
    "            vp_string = normalize_verb_span(verb_bunsetu_span)\n",
    "            if not vp_string:\n",
    "                print(\n",
    "                    \"Error normalizing verb phrase:\",\n",
    "                    verb_bunsetu_span,\n",
    "                    \"in document\",\n",
    "                    doc,\n",
    "                )\n",
    "                continue\n",
    "            matches.append(\n",
    "                (\n",
    "                    noun.norm_,\n",
    "                    case_particle.norm_,\n",
    "                    # verb.norm_,\n",
    "                    vp_string,\n",
    "                )\n",
    "            )\n",
    "    return matches\n",
    "\n",
    "\n",
    "class TestExtraction(unittest.TestCase):\n",
    "    def test_npv(self):\n",
    "        self.assertEqual(\n",
    "            npv_matcher(nlp(example_sentence)),  # 東京では，銀座でランチをたべよう。\n",
    "            [(\"銀座\", \"で\", \"食べる\"), (\"ランチ\", \"を\", \"食べる\")],\n",
    "        )\n",
    "        self.assertEqual(npv_matcher(nlp(\"京都にも行く。\")), [])\n",
    "        self.assertEqual(\n",
    "            npv_matcher(nlp(\"ことを説明するならば\")), [(\"こと\", \"を\", \"説明する\")]\n",
    "        )\n",
    "        # ここは「ことになる」あるいは「ことにならない」が正しいが，GiNZAではこれがイディオム処理（fixed/compound）のため，「ざるをえない」などと一緒に処理すべき\n",
    "        self.assertEqual(\n",
    "            npv_matcher(nlp(\"ことにならない\")), [(\"こと\", \"に\", \"ならない\")]\n",
    "        )\n",
    "\n",
    "\n",
    "unittest.main(\n",
    "    argv=[\"ignored\", \"-v\", \"TestExtraction.test_npv\"], verbosity=2, exit=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npv_matcher(nlp(\"彼が語ります\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npv_matcher(nlp(\"ことを説明するならば\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp(\"ことにならない\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npv_matcher(nlp(\"ことにならない\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## TED トークコーパスの作成\n",
    "\n",
    "Hugginface の datasets を使って，TED トークの日本語に翻訳された字幕をコーパス化します。\n",
    "データセットのページは以下：\n",
    "\n",
    "- <https://huggingface.co/datasets/ted_talks_iwslt>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ted_dataset_2014 = load_dataset(\n",
    "    \"ted_talks_iwslt\", language_pair=(\"en\", \"ja\"), year=\"2014\"\n",
    ")\n",
    "ted_dataset_2015 = load_dataset(\n",
    "    \"ted_talks_iwslt\", language_pair=(\"en\", \"ja\"), year=\"2015\"\n",
    ")\n",
    "ted_dataset_2016 = load_dataset(\n",
    "    \"ted_talks_iwslt\", language_pair=(\"en\", \"ja\"), year=\"2016\"\n",
    ")\n",
    "\n",
    "ted_dataset_2017jaen = load_dataset(\"iwslt2017\", \"iwslt2017-ja-en\")\n",
    "# en-jaとja-enは同じデータなので，一方のみ使う\n",
    "# ted_dataset_2017enja = load_dataset(\"iwslt2017\", \"iwslt2017-en-ja\")\n",
    "# set(t[\"ja\"] for t in ted_dataset_2017jaen[\"train\"][\"translation\"]) == set(t[\"ja\"] for t in ted_dataset_2017enja[\"train\"][\"translation\"]) ==> True, same data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge corpus and limit to reasonable size (30k lines)\n",
    "ted_corpus: list[str] = (\n",
    "    [d[\"ja\"] for d in ted_dataset_2014[\"train\"][\"translation\"]]\n",
    "    + [d[\"ja\"] for d in ted_dataset_2015[\"train\"][\"translation\"]]\n",
    "    + [d[\"ja\"] for d in ted_dataset_2016[\"train\"][\"translation\"]]\n",
    "    + [d[\"ja\"] for d in ted_dataset_2017jaen[\"train\"][\"translation\"]][\n",
    "        :30000\n",
    "    ]  # > 200k items, so limit to first 30k\n",
    ")\n",
    "len(ted_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicated entries\n",
    "ted_corpus = list(\n",
    "    dict.fromkeys(ted_corpus)\n",
    ")  # list(set(ted_corpus))は順番を変えるので，現れる順位を優先するdict.fromkeysを使う\n",
    "len(ted_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove title-like entries\n",
    "ted_corpus = [\n",
    "    s for s in ted_corpus if not re.match(r\"[・゠-ヿ]+(\\s*「|：|: ).{0,40}$\", s)\n",
    "]\n",
    "len(ted_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove non-speech-like entries\n",
    "# Slow version:\n",
    "# ted_corpus = [\n",
    "#     s\n",
    "#     for s in ted_corpus\n",
    "#     if set(t.norm_ for t in nlp(s, disable=[\"ner\", \"dep\"])).intersection({\"です\", \"ます\"})\n",
    "# ]\n",
    "\n",
    "ted_corpus = [\n",
    "    s\n",
    "    for s in ted_corpus\n",
    "    if re.search(\n",
    "        r\"です|でした|でしょう?|でして|ます|ません|ました|ましょう?|まして|(下|くだ)さい\",\n",
    "        s,\n",
    "    )\n",
    "]\n",
    "\n",
    "len(ted_corpus)\n",
    "\n",
    "# GPU (A4000): ja_ginza 4m13s / ja_ginza_bert_large 9m6s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# （TEDxUdeMで撮影）のようなコメントを削除\n",
    "\n",
    "ted_corpus = [re.sub(r\"\\s*[（(][^）)]+[）)]\\s*\", \"\", s) for s in ted_corpus]\n",
    "len(ted_corpus)  # 文章内から削除のため，数は同じ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ted_corpus[:20]  # Check the first 20 paragraphs in ted_corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/ted_corpus.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"\\n\".join(ted_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp(\n",
    "    \"トビー・エクルズは、この状況を覆すための画期的なアイデア「ソーシャル・インパクト・ボンド（社会インパクト債権）」について話します。\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp(\n",
    "    \"野生生物の保護に尽力するボイド・ヴァーティは「自然の大聖堂は人間性の最高の部分を映し出してくれる鏡である」と話します。\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp(\"チャンの素晴らしい手作りの弓がしなる様子をご堪能ください。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp(\n",
    "    \"アメリカ人が「共有する」市民生活は、どれだけお金を持っているかによって違うものになってしまったと言っていいでしょう。\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp(\n",
    "    \"しかし飛行機や自動車が生まれて100年がたった今も、それが本当に実現されたことはありませんでした。\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# バグ\n",
    "pp(\n",
    "    \"TEDxTC でジョナサン・フォーリーが「テラカルチャー」（地球全体のための農業）に取り組む必要性を訴えます。\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp(\n",
    "    \"感動のトーク　マッカーサー賞受賞者である活動家のマジョラ・カーターが サウスブロンクスの環境正義を求める闘いについて詳しく説明し 都市政策の欠陥  マイノリティ地区に最大の被害を受けることを示します\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ginza.bunsetu_spans(\n",
    "    nlp(\n",
    "        \"感動のトーク　マッカーサー賞受賞者である活動家のマジョラ・カーターが サウスブロンクスの環境正義を求める闘いについて詳しく説明し 都市政策の欠陥  マイノリティ地区に最大の被害を受けることを示します\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npv_matcher(\n",
    "    nlp(\n",
    "        \"感動のトーク　マッカーサー賞受賞者である活動家のマジョラ・カーターが サウスブロンクスの環境正義を求める闘いについて詳しく説明し 都市政策の欠陥  マイノリティ地区に最大の被害を受けることを示します\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## コーパスからの抽出処理\n",
    "\n",
    "処理する文章が多いときは`nlp.pipe()`を使い，文字列のリストを引数にすることで，並列処理が行えます。\n",
    "そこから得られた doc(s) を npv_matcher に渡し，chain.from_iterable でくっつけます。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "\n",
    "ted_npvs = list(chain.from_iterable(npv_matcher(doc) for doc in nlp.pipe(ted_corpus)))\n",
    "# GPU (A4000): 47s   (ja_ginza) / 1m3s  (ja_ginza_electra) / 3m40s (ja_ginza_bert_large)\n",
    "# CPU (3960x): 1m31s (ja_ginza) / 5m42s (ja_ginza_electra) / 13m30s (ja_ginza_bert_large)\n",
    "# M1 (GPU)   : 50s   (ja_ginza) / 14m33s (ja_ginza_bert_large)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ted_npvs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 格助詞ごとの項目数を調べるなら\n",
    "from collections import Counter\n",
    "\n",
    "Counter(npv[1] for npv in ted_npvs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## NPV データの保存\n",
    "\n",
    "検索インターフェースでは今回 NPV パターンのみを検索するため，そのデータのみを CSV 形式に書き出す。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "data_dir = Path(\"../data/\")\n",
    "data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_records(ted_npvs, columns=[\"n\", \"p\", \"v\"])\n",
    "df[\"corpus\"] = \"TED\"\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(data_dir / f\"ted_npvs_{model_name}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    with open(data_dir / \"jnlp-sample-3000.txt\", encoding=\"utf-8\") as f:\n",
    "        jnlp_corpus = f.readlines()\n",
    "except FileNotFoundError:\n",
    "    with open(data_dir / \"jnlp-sample-3000-python.txt\", encoding=\"utf-8\") as f:\n",
    "        jnlp_corpus = f.readlines()\n",
    "\n",
    "jnlp_npvs = list(chain.from_iterable(npv_matcher(doc) for doc in nlp.pipe(jnlp_corpus)))\n",
    "# GPU (A4000): 32s   (ja_ginza) / 1m6s (ja_ginza_electra) / 2m25s (ja_ginza_bert_large)\n",
    "# CPU (3960x): 1m42s (ja_ginza) / 6m2s (ja_ginza_electra)\n",
    "# M1 (GPU)   : 45s   (ja_ginza) / 10m23s (ja_ginza_bert_large)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j_df = pd.DataFrame.from_records(jnlp_npvs, columns=[\"n\", \"p\", \"v\"])\n",
    "j_df[\"corpus\"] = \"自然言語処理\"\n",
    "j_df.to_csv(f\"../data/jnlp_npvs_{model_name}.csv\", index=False)\n",
    "j_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp(\n",
    "    \"共参照関係認定基準1を用いた場合と共参照関係認定基準2を用いた場合とを比較すると，共参照関係認定基準2の方が厳しい制約であるため，再現率が低下するかわりに，適合率が上昇している．\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 他ツール\n",
    "\n",
    "KeyWord-In-Context (KWIC)を使い，コーパスを検索できます。\n",
    "\n",
    "ここでのKWIC検索は任意のコーパス（ted_corpusなど，文字列のリスト）と任意の正規表現を渡し，出力されるのが\n",
    "\n",
    "| キーワードまでの文字列 | キーワード | キーワード後の文字列 |\n",
    "|------------------------|------------|----------------------|\n",
    "\n",
    "という表です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from re import Pattern\n",
    "\n",
    "\n",
    "def kwic(corpus: list[str], query: str | Pattern[str]) -> list[tuple[str, str, str]]:\n",
    "    \"\"\"`corpus`に対し，`query`文字列（正規表現）を用いて検索語とその前後の文字列を分けて返す。\n",
    "    それを用いてKeyWord-In-Context表示ができる。\"\"\"\n",
    "\n",
    "    matches: list[tuple[str, str, str]] = []\n",
    "\n",
    "    for text in corpus:\n",
    "        for match in re.finditer(query, text):\n",
    "            before_idx, after_idx = match.span()\n",
    "            before = text[:before_idx]\n",
    "            matched = text[before_idx:after_idx]\n",
    "            after = text[after_idx:]\n",
    "            matches.append((before, matched, after))\n",
    "\n",
    "    return matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\n",
    "    \"display.max_colwidth\", None\n",
    ")  # Allow strings in columns to show fully without being ellipsed.\n",
    "pd.DataFrame.from_records(\n",
    "    kwic(ted_corpus, r\"文字通り\"), columns=[\"before\", \"keyword\", \"after\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
